---
layout: post
title: "Nesting interpreters"
date: 2026-01-16
---

# Nesting interpreters

A friend of mine enquired about my experience with LÃ©vy-optimal evaluation of lambda calculus,
to which I replied:

```
I've tried various others and found them horribly slow
on actual benchmarks despite their optimality.
So I prefer simple (and hence non-opimal) runtimes that are pretty
quick in practice, namely combinator graph reduction such as
https://github.com/augustss/MicroHs or my own
https://github.com/tromp/AIT/blob/master/uni.c

There's one thing that I really would like to get more optimal though.
What I notice with combinatory reducers is that you can run nested
binary combinatory logic interpreters
with only additive overhead; each additional interpreter basically
rewrites itself into the higher level one.
That is not the case with all the binary lambda calculus interpreters
I tried, even the ones based on
Kiselyov's direct translation
https://theory.stanford.edu/~blynn/lambda/kiselyov.html

I'd like to know if that is fundamentally impossible or not.
```

My friend understandably asked for clarification on the nesting of interpreters and additive overhead,
and while pondering a reply, I realized this might be of interest to more people, so I decided to
make a blog post about it.

## Universal Lambda Machine

To dive straight in, I've proposed a ULM [1] [2] that reads from a binary input stream
an encoded lambda term, and applies it to the remainder of input.

When the encoded term is itself a lambda calculus interpreter operating as a universal lambda machine,
then the ULM's behaviour on the remaining input is identical to its original behaviour.

Except when behaviour includes performance on actual hardware.
Nesting interpreters normally incur a constant factor slowdown or worse.

This was already noted in my 2012 IOCCC submission where a "Performnance" section [4]
compares runtimes with 0,1,2,3, and 4 nested interpreters running a prime number sieve,
incurring slowdowns of respectively 4.4x, 10.0x, 12.2x, and 12.4x.

Let's rerun that experiment with the much more performant (and compilable) uni.c runtime [4],
and with a simpler end-task resulting in a normal form:

```
$ (cat delimit.blc; echo -n 1111000111001) | ../uni -b
11010steps 9746 time 0ms steps/s 666M #GC 0 HP 20492
$ (cat uni.blc delimit.blc; echo -n 1111000111001) | ../uni -b
11010steps 143176 time 3ms steps/s 47M #GC 0 HP 159806
$ (cat uni.blc uni.blc delimit.blc; echo -n 1111000111001) | ../uni -b
11010steps 1554114 time 36ms steps/s 43M #GC 1 HP 539112
$ (cat uni.blc uni.blc uni.blc delimit.blc; echo -n 1111000111001) | ../uni -b
11010steps 17928565 time 407ms steps/s 44M #GC 18 HP 282330
$ (cat uni.blc uni.blc uni.blc uni.blc delimit.blc; echo -n 1111000111001) | ../uni -b
11010steps 209071006 time 5047ms steps/s 41M #GC 233 HP 518110
$ (cat uni.blc uni.blc uni.blc uni.blc uni.blc delimit.blc; echo -n 1111000111001) | ../uni -b
11010steps 2439995720 time 69617ms steps/s 35M #GC 3407 HP 940096
```

